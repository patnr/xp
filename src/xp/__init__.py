import shutil
import sys
import re
import subprocess
import time
from datetime import datetime
from pathlib import Path
from tqdm.auto import tqdm
from tempfile import NamedTemporaryFile

import dill

from . import uplink
from .launch_xps import mp

timestamp = "%Y-%m-%d_at_%H-%M-%S"
responsive = dict(check=True, capture_output=True, text=True)


def find_latest_run(root: Path):
    """Find the latest experiment (dir containing many)"""
    lst = []
    for f in root.iterdir():
        try:
            f = datetime.strptime(f.name, timestamp)
        except ValueError:
            pass
        else:
            lst.append(f)
    f = max(lst)
    f = datetime.strftime(f, timestamp)
    return f


def git_dir():
    """Get project (.git) root dir and HEAD 'sha'."""
    git_dir = subprocess.run(["git", "rev-parse", "--show-toplevel"], **responsive).stdout.strip()
    return Path(git_dir)


def git_sha():
    """Get project HEAD 'sha'."""
    return subprocess.run(["git", "rev-parse", "--short", "HEAD"], **responsive).stdout.strip()


def mk_data_dir(
    data_dir,
    tags=tuple(),  # Whatever you want, e.g. "v1"
    mkdir=True,  # Make dirs, including xps/ and res/
):
    """Add timestamp/tag and mkdir for data storage."""
    if tags:
        data_dir /= tags
    else:
        data_dir /= datetime.now().strftime(timestamp)

    if mkdir:
        data_dir.mkdir(parents=True)
        (data_dir / "xps").mkdir()
        (data_dir / "res").mkdir()

    return data_dir


def prj_dir(script: Path):
    """Find python project's root dir.

    Returns the (shallowest) parent below `script`
    of first found among some common root markers.
    """
    markers = ["pyproject.toml", "requirements.txt", "setup.py", ".git"]
    for d in script.resolve().parents:
        for marker in markers:
            candidate = d / marker
            if candidate.exists():
                return d


def save(xps, data_dir, nBatch):
    print(f"Saving {len(xps)} xp's to", data_dir)
    batch_size = 1 + len(xps) // nBatch

    def save_batch(i):
        xp_batch = xps[i * batch_size : (i + 1) * batch_size]
        (data_dir / "xps" / str(i)).write_bytes(dill.dumps(xp_batch))

    # saving can be slow ⇒ mp
    mp(save_batch, range(nBatch))


def dispatch(
    fun: callable,
    xps: list,
    host: str = None,  # Server alias
    script: Path = None,  # Path to script containing `fun`
    nCPU: int = None,  # number of CPUs to engage
    nBatch: int = None,  # number of batches (splits) of xps
    # NB: `multiprocessing` module already does "chunking",
    # so this is intended to be used on clusters with queue systems.
    # For efficiency, the resulting batch_size should be >= nCPU (per node) * 100
    proj_dir: Path = None,  # e.g. Path(__file__).parents[0]
    data_root: Path = Path.home() / "data",
    data_root_on_remote: Path = "~/data",
):
    """
    Run `fun` on `xps` on various different hosts.
    This function can be replaced as follows:

    >>> results = [experiment(**kwargs) for kwargs in xps]
    ... # Optionally, save:
    ... (data_root / data_dir / "xps").write_bytes(dill.dumps(xps))
    ... (data_root / data_dir / "res").write_bytes(dill.dumps(results))

    The `proj_dir` must be a parent to `script`,
    and gets copied into (and so uploaded with) `data_dir` (which also mirrors path of `proj_dir`!).
    To promote independence of the uploaded code "environment" vs. whatever
    "happens to be" the `cwd` (less headaches!), the `proj_dir` should NOT be the `cwd`.
    Still, if possible (if subpath to `proj_dir`), the `cwd` is "preserved" on remote,
    such that resources specified relative to it (sloppy!) may be found.
    """
    # Don't want to pickle `fun`, because it often contains very deep references,
    # and take up a lot of storage (especially if saved with each xp).
    # ⇒ Ensure we know the script from which we can import it.
    # TODO: isn't it supposed to be possible to only pickle references?
    if script is None:
        script = fun.__module__
        if script == "__main__":
            script = fun.__code__.co_filename
    script = Path(script)

    if proj_dir is None:
        proj_dir = prj_dir(script)
    if len(proj_dir.relative_to(Path.home()).parts) <= 2:
        msg = f"The `proj_dir` ({proj_dir}) will be uploaded, but is too close to home dir."
        raise RuntimeError(msg)

    data_dir = data_root / proj_dir.stem / script.relative_to(proj_dir).stem
    data_dir = mk_data_dir(data_dir)

    # Host alias "glob"
    if host is None:
        host = "SUBPROCESS"
    elif host.endswith("*"):
        for line in (Path("~").expanduser() / ".ssh" / "config").read_text().splitlines():
            if line.startswith("Host " + host[:-1]):
                host = line.split()[1]
                break

    # Place launch script in same dir as script
    shutil.copy(Path(__file__).parent / "launch_xps.py", script.parent)

    # Save xps -- partitioned (for node distribution)
    if nBatch is None:
        nBatch = 40 if host.startswith("login-") else 1
    save(xps, data_dir, nBatch)

    # List resulting paths
    paths_xps = sorted((data_dir / "xps").iterdir(), key=lambda p: int(p.name))
    assert paths_xps, f"No files found in {data_dir}"

    # Run locally via subprocess
    if host == "SUBPROCESS":
        for xp in paths_xps:
            try:
                # current_interpreter = "python" # requires active venv
                current_interpreter = sys.executable
                subprocess.run(
                    [
                        current_interpreter,
                        script.parent / "launch_xps.py",
                        script.stem,
                        fun.__name__,
                        xp,
                        str(nCPU),
                    ],
                    check=True,
                    cwd=Path.cwd(),
                )
            except subprocess.CalledProcessError:
                raise

    # Run on NORCE HPC cluster with SLURM queueing system
    # elif "hpc.intra.norceresearch" in host:
    #     remote = uplink.Uplink(host)
    #
    #     if data_root_on_remote is None:
    #         data_root_on_remote = remote.cmd("env | grep USERWORK").stdout.splitlines()[0].split("=")[1]
    #     data_dir_remote = data_root_on_remote / data_dir.relative_to(data_root)
    #
    #     with remote.sym_sync(data_dir_remote, data_dir_remote, cwd):
    #         # Install (potentially outdated) deps (from lockfile)
    #         # PS: Pre-install `uv` using `wget -qO- https://astral.sh/uv/install.sh | sh`
    #         venv = f"~/.cache/venvs/{cwd.name}"
    #         remote.cmd(
    #             f"command cd {data_dir_remote / cwd.name}; UV_PROJECT_ENVIRONMENT={venv} uv sync",
    #             capture_output=False,
    #         )
    #
    #         # Send job submission script
    #         with NamedTemporaryFile(mode="w+t", delete_on_close=False) as sbatch:
    #             txt = (Path(__file__).parent / "slurm_script.sbatch").read_text()
    #             txt = eval(f"f'''{txt}'''", {}, locals())
    #             sbatch.write(txt)
    #             sbatch.close()
    #             remote.rsync(sbatch.name, data_dir_remote / "job_script.sbatch")
    #
    #         # Submit
    #         job_id = remote.cmd(f"command cd {data_dir_remote}; sbatch job_script.sbatch")
    #         print(job_id.stdout, end="")
    #         job_id = int(re.search(r"job (\d*)", job_id.stdout).group(1))
    #
    #         # Monitor job progress
    #         nJobs = len(paths_xps)
    #         with tqdm(total=nJobs, desc="Jobs") as pbar:
    #             unfinished = nJobs
    #             while unfinished:
    #                 time.sleep(1)  # dont clog the ssh uplink
    #                 new = f"squeue -j {job_id} -h -t pending,running -r | wc -l"
    #                 new = int(remote.cmd(new).stdout)
    #                 inc = unfinished - new
    #                 pbar.update(inc)
    #                 unfinished = new
    #
    #         # Provide error summary
    #         failed = f"sacct -j {job_id} --format=JobID,State,ExitCode,NodeList | grep -E FAILED"
    #         failed = remote.cmd(failed, check=False).stdout.splitlines()
    #         if failed:
    #             regex = r"_(\d+).*(node-\d+) *$"
    #             nodes = {int((m := re.search(regex, ln)).group(1)): m.group(2) for ln in failed}
    #             for task in nodes:
    #                 print(f" Error for job {job_id}_{task} on {nodes[task]} ".center(70, "="))
    #                 print(remote.cmd(f"cat {data_dir_remote}/error/{task}").stdout)
    #             raise RuntimeError(f"Task(s) {list(nodes)} had errors, see printout above.")

    # Run on some other remote server
    # NOTE:
    # - See xp/setup-compute-node.sh for instructions on setting up a GCP VM.
    # - Use "localhost" for testing/debugging w/o actual server.
    else:
        remote = uplink.Uplink(host)

        data_dir_remote = data_root_on_remote / data_dir.relative_to(data_root)
        paths_xps = [data_dir_remote / xp.relative_to(data_dir) for xp in paths_xps]

        # Make (try!) cwd such that the relative path of the script is same as locally
        try:
            cwd = Path.cwd().relative_to(proj_dir)
        except ValueError:
            print(
                "Warning: The cwd is outside of the project path."
                "But if your script is well crafted, everything should still work."
            )
            cwd = Path(".")
        finally:
            cwd = data_dir_remote / proj_dir.stem / cwd
        script = data_dir_remote / proj_dir.stem / script.relative_to(proj_dir)

        with remote.sym_sync(data_dir_remote, data_dir, proj_dir):
            # Install (potentially outdated) deps (from lockfile)
            # PS: Pre-install `uv` using `wget -qO- https://astral.sh/uv/install.sh | sh`
            venv = f"~/.cache/venvs/{proj_dir.stem}"
            remote.cmd(
                f"cd {data_dir_remote / proj_dir.stem}; UV_PROJECT_ENVIRONMENT={venv} uv sync",
                capture_output=False,  # simply print
            )

            # Run (`launch_xps.py` uses `mp` ⇒ no point parallelising this loop)
            for xp in paths_xps:
                remote.cmd(
                    [
                        # PS: A well-crafted script should be independend of cwd ...
                        f"cd {cwd};",  # ... so should ideally be able to comment out this line.
                        f"{venv}/bin/python",
                        script.parent / "launch_xps.py",
                        script.stem,
                        fun.__name__,
                        xp,
                        nCPU,
                    ],
                    capture_output=False,  # simply print
                )
    return data_dir
